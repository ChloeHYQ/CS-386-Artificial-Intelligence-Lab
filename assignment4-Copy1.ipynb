{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4 (Intro to AI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marks total is 12: Question 7 is worth 6 marks and the other questions are weighted equally (1 mark each)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Dealing with data Pythonically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You might find these packages useful. You may import any others you want!\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. The file `\"data/ads_dataset.tsv\"` uses tabs instead of commas to separate fields. Load the data set `\"data/ads_dataset.tsv\"` into a Python `pandas` data frame called `ads`. If you are using the `read_csv()` function you will need to know that `\"\\t\"` means tab when trying to set the delimeter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place your code here\n",
    "ads = pd.read_csv(\"data/ads_dataset.tsv\",sep='\\t')\n",
    "ads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Write a Python function called `getDfSummary()` that does the following:\n",
    "- Takes as input a data frame\n",
    "- Creates a new data frame called `output_data` that:\n",
    "  - Consists of the output of the `pandas` function `describe()`. (*Hint: Check the `pandas` [(manual page)](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html) for this function if you want to learn more.*)\n",
    "  - The data frame `output_data` should have the original feature names from `input_data` as **rows**. (*Hint: Are they rows by default? If not, how could you make them rows?*)\n",
    "  - Adds a column called `spread` that is the result of subtracting the `min` from the `max`. (*Hint: What does the `describe()` function return? Do we have a column called `max` and `min`?*)\n",
    "- Returns the new `output_data` data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDfSummary(input_data):\n",
    "    output_data =ads.describe()\n",
    "    output_data=output_data.T\n",
    "    output_data['spread'] =output_data['max'] -output_data['min']\n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. How long does it take for your `getDfSummary()` function to work on your `ads` data frame? Show us the results below.\n",
    "\n",
    "*Hint: `%timeit getDfSummary(ads)`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place your code here\n",
    "%timeit getDfSummary(ads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Show the output of the `getDfSummary()` function you created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Place your code here\n",
    "getDfSummary(ads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Using the results returned from `getDfSummary()`, which fields, if any, have a spread of 1? Full credit will be given for a code based approach that prints out the correct field names. Partial credit will be given if you simply manually explore the results of `getDfSummary()`. Print each field name on a line by itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Place your code here\n",
    "df=getDfSummary(ads)\n",
    "df=df[df.spread==1]\n",
    "df.iloc[:,1:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. For the fields that have a spread of 1, what percentage of records have the value of `1`? You may assume that if a field had a spread of 1 that it only has values 1 and 0.\n",
    "\n",
    "Full credit will be given for a code based approach that prints out the correct field names **and** their respective percentages. Partial credit will be given if you simply manually explore the results of `getDfSummary()`. (*Hint: Using the `pandas` function `.get_value()` may be useful if you happen to know the row and column name you want.*)\n",
    "\n",
    "Your results should look something like this:\n",
    "\n",
    "`The field` **`xxx`** `has` **`yyy`** `percent of its records set to 1.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Place your code here\n",
    "df1=df[df == 1]\n",
    "df2=df1.fillna(value=0)\n",
    "df3=df2.mean(1)\n",
    "for i in range(len(df3)):\n",
    "    print('The field {0} has {1} percent of its records set to 1.'.format(df3.index[i],df3[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2. Functions, loops and plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Write a function called `squared_less_100()` that takes a single number, squares it, and then returns the square minus 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def squared_less_100(input_number):\n",
    "    output_number = np.square(input_number)-100\n",
    "    # Code here!\n",
    "    return output_number\n",
    "\n",
    "#squared_less_100(100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Create three empty lists called `sl1_results`, `absolute_results`, `log_results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl1_results = []\n",
    "absolute_results =[]\n",
    "log_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Create a list of integers from 1 to 20 (including both 1 and 20) called `original_set`. Loop through this list and apply your `squared_less_100()` function, the `numpy` function for finding absolute value, and the `numpy` natural log function. Store the result of each function by appending it the the initially empty lists you created in Part 4 Question 2. (*Hint: If you don't know what the `numpy` functions for absolute value or natural logs are, simply search online and it will be one of the first results. Although, you can probably guess what they would be!*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Code here!\n",
    "import numpy as np\n",
    "original_set=[x for x in range(1,21)]\n",
    "for i in range(len(original_set)):\n",
    "    org=squared_less_100(original_set[i])\n",
    "    sl1_results.append(org)\n",
    "    absolute_results.append(np.fabs(original_set[i]))\n",
    "    log_results.append(np.log(original_set[i]))\n",
    "print(sl1_results)\n",
    "print(absolute_results)\n",
    "print(log_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Create a plot of your results. On the x-axis put `original_set`. Now, all within the **same plot** put a line for the `sl1_results`, a line for `absolute_results`, and a line for `log_results`. Don't forget to add a legend so that I know which line is for which set of results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "x=original_set\n",
    "y=sl1_results\n",
    "z=absolute_results\n",
    "h=log_results\n",
    "plt.plot(x, y,linestyle=\"--\", color=\"orange\")\n",
    "plt.plot(x, z,color=\"red\")\n",
    "plt.plot(x, h)\n",
    "plt.legend([\"sl1_results\",\"sl1_results\",\"log_results\"], loc=\"upper right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# Code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example we will be using the data/cell2cell.csv data. We will practice modelling a problem looking at the likelyhood of customers leaving a company (see below). The idea of this question is to go through the main steps to implement a solution in sklearn and observe similarities in implementing different algorithms.\n",
    "\n",
    "These historical data consist of 31,892 customers: 15,855 customers that churned (i.e., left the company) and 16,036 that did not churn. Here are the data set's 12 columns:\n",
    "\n",
    "Col.  Var. Name  Var. Description\n",
    "----- ---------- --------------------------------------------------------------\n",
    "1     revenue    Mean monthly revenue in dollars\n",
    "2     outcalls   Mean number of outbound voice calls\n",
    "3     incalls    Mean number of inbound voice calls\n",
    "4     months     Months in Service\n",
    "5     eqpdays    Number of days the customer has had his/her current equipment\n",
    "6     webcap     Handset is web capable\n",
    "7     marryyes   Married (1=Yes; 0=No)\n",
    "8     travel     Has traveled to non-US country (1=Yes; 0=No)\n",
    "9     pcown      Owns a personal computer (1=Yes; 0=No)\n",
    "10    creditcd   Possesses a credit card (1=Yes; 0=No)\n",
    "11    retcalls   Number of calls previously made to retention team\n",
    "12    churndep   Did the customer churn (1=Yes; 0=No)\n",
    "The first 11 columns are our attributes/features. The last column, \"churndep\", is the target variable.\n",
    "\n",
    "For this question apply any 3 machine learning algorithms that have covered in class (e.g. support vector machines, decision trees, logistic regression, nearest neighbour, neural network). Practice following the example and report your results for each algorithm in the form of:\n",
    "(1) accuracy in correctly predicting churn\n",
    "(2) a confusion matrix \n",
    "\n",
    "You can follow the template provided for coding. Notice we go through the same steps for each algorithm:\n",
    "- load data\n",
    "- create test and train data\n",
    "- instantiate model\n",
    "- train on training data examples\n",
    "- test on out sample data\n",
    "- review results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report \n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Read in our csv data\n",
    "data = pd.read_csv('data/cell2cell.csv')\n",
    "\n",
    "# Put all features into X and the target variable into Y\n",
    "X = data.drop('churndep',axis=1)\n",
    "Y = data['churndep']\n",
    "# Prepare to do some training and testing\n",
    "\n",
    "tree_accuracies = []\n",
    "logistic_accuracies = []\n",
    "svm_accuracies=[]\n",
    "\n",
    "# Loop through your training percentages, split your data with each percentage, \n",
    "#  create both models, fit/train both models, predict with your models and \n",
    "#  append each accuracy to the correct list\n",
    "for i in [0.5]:\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = i)\n",
    "    # Create all the models\n",
    "    tree = DecisionTreeClassifier(criterion = 'entropy')\n",
    "    logistic = LogisticRegression()\n",
    "    clf = svm.SVC(kernel='rbf')\n",
    "\n",
    "    # Fit the models\n",
    "    tree.fit(x_train, y_train)\n",
    "    logistic.fit(x_train, y_train)\n",
    "    clf.fit(x_train,y_train)\n",
    "\n",
    "    # Fit all the models (Test)\n",
    "    Y_test_predicted_tree = tree.predict(x_test)\n",
    "    Y_test_predicted_logistic = logistic.predict(x_test)\n",
    "    Y_test_predicted_svm = clf.predict(x_test)\n",
    "\n",
    "\n",
    "    # report results and validate\n",
    "\n",
    "    # Get the accuracy for the models' predictions\n",
    "    tree_acc = accuracy_score( y_test,Y_test_predicted_tree,)\n",
    "    tree_accuracies.append(tree_acc)\n",
    "    logistic_acc = accuracy_score(y_test,Y_test_predicted_logistic)\n",
    "    logistic_accuracies.append(logistic_acc)\n",
    "    svm_acc = accuracy_score(Y_test_predicted_svm, y_test)\n",
    "    svm_accuracies.append(svm_acc)\n",
    "    \n",
    "\n",
    "    # get the confusion matrix\n",
    "    tree_matrix=confusion_matrix(Y_test_predicted_tree, y_test)\n",
    "    logistic_matrix=confusion_matrix(Y_test_predicted_logistic, y_test)\n",
    "    svm_matrix=confusion_matrix(Y_test_predicted_svm, y_test)\n",
    "\n",
    "# print accuracy and confusion matrix in a nicely formatted table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4. Calculating Entropy and Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate entropy and information gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Consider this population. What is the entropy? \n",
    "<br /><img src=\"images/q5-1.png\" height=140px width=142px style=\"float: left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put your answer here and working out. No working out = no mark. This question has no coding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Consider these possible splits at the top of a decision tree. Calculate the information gain obtained with Split A and Split B. Which split is better and why?\n",
    "<br /><img src=\"images/q5-a.png\" height=300px width=300px style=\"float: left; padding-right: 20px;\"/> <img src=\"images/q5-b.png\" height=300px width=300px style=\"float: left; padding-left: 20px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put your answer here and working out. No working out = no mark. This question has no coding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5. Learning Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example we will be using the `data/cell2cell.csv` data. It is a dataset to predict customer churn (chance of customer leaving a phone company) based on various inputs describing the customer and their usage of the service.\n",
    "\n",
    "These historical data consist of 31,892 customers: 15,855 customers that churned (i.e., left the company) and 16,036 that did not churn. Here are the data set's 12 columns:\n",
    "\n",
    "```\n",
    "Col.  Var. Name  Var. Description\n",
    "----- ---------- --------------------------------------------------------------\n",
    "1     revenue    Mean monthly revenue in dollars\n",
    "2     outcalls   Mean number of outbound voice calls\n",
    "3     incalls    Mean number of inbound voice calls\n",
    "4     months     Months in Service\n",
    "5     eqpdays    Number of days the customer has had his/her current equipment\n",
    "6     webcap     Handset is web capable\n",
    "7     marryyes   Married (1=Yes; 0=No)\n",
    "8     travel     Has traveled to non-US country (1=Yes; 0=No)\n",
    "9     pcown      Owns a personal computer (1=Yes; 0=No)\n",
    "10    creditcd   Possesses a credit card (1=Yes; 0=No)\n",
    "11    retcalls   Number of calls previously made to retention team\n",
    "12    churndep   Did the customer churn (1=Yes; 0=No)\n",
    "```\n",
    "\n",
    "The first 11 columns are our attributes/features. The last column, `\"churndep\"`, is the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a decision tree using entropy with no max depth and a logistic regression. \n",
    "\n",
    "In both cases assign 10% to 90% of the data to training (in increments of 10%) and the rest to test. \n",
    "\n",
    "For each training set,fit both models and then get the accuracy on the remaining data used for test. \n",
    "\n",
    "Lastly, create a plot:\n",
    "Where the x-axis includes 0.10 to 0.90 (one tick for each training data percentage) and the y-axis indicates accuracy. Your plot should have two lines/curves on it: one for the decision tree and one for logistic regression. \n",
    "\n",
    "You can use the following code template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Read in our csv data\n",
    "data = pd.read_csv('data/cell2cell.csv')\n",
    "\n",
    "# Put all features into X and the target variable into Y\n",
    "X = data.drop('churndep', axis = 1)\n",
    "Y = data['churndep']\n",
    "\n",
    "# Prepare to do some training and testing\n",
    "training_percentages = [0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90]\n",
    "tree_accuracies = []\n",
    "logistic_accuracies = []\n",
    "\n",
    "# Loop through your training percentages, split your data with each percentage, \n",
    "#  create both models, fit/train both models, predict with your models and \n",
    "#  append each accuracy to the correct list\n",
    "for training_percentage in training_percentages:\n",
    "    train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=training_percentage)\n",
    "    \n",
    "    # Create both models\n",
    "    tree =DecisionTreeClassifier(criterion='entropy')\n",
    "    logistic =LogisticRegression()\n",
    "    \n",
    "    # Fit both models\n",
    "    tree = tree.fit(train_X, train_Y)\n",
    "    logistic = logistic.fit(train_X, train_Y)\n",
    "    \n",
    "    # Get predictions from both models\n",
    "    Y_test_predicted_tree = tree.predict(test_X)\n",
    "    Y_test_predicted_logistic = logistic.predict(test_X)\n",
    "\n",
    "    # Get the accuracy for the models' predictions\n",
    "    tree_acc = accuracy_score(test_Y, Y_test_predicted_tree)\n",
    "    logistic_acc = accuracy_score(test_Y, Y_test_predicted_logistic)\n",
    "    \n",
    "    # Now that I have a tree and logistic accuracy, I should add them to my list of accuracies\n",
    "    tree_accuracies.append(tree_acc)\n",
    "    logistic_accuracies.append(logistic_acc)\n",
    "    \n",
    "# Plot two curves on one plot. Don't forget labels and your legend\n",
    "plt.plot(training_percentages, tree_accuracies, label=\"decision tree\")\n",
    "plt.plot(training_percentages, logistic_accuracies, label=\"logistic\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"train percentage\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What type of plot is this?\n",
    "\n",
    "Do you observe any interesting trends in the plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "put your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6. Cross Validation, Fitting Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This question uses the same dataset as the previous question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Review the below code which implements a decision and performs cross validation based on changing the max depth complexity parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas to read in data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import matplotlib for plotting\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Import decision trees and logistic regression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import train, test, and evaluation functions\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "# Read data using pandas\n",
    "data = pd.read_csv(\"data/cell2cell.csv\")\n",
    "\n",
    "# Split into X and Y\n",
    "X = data.drop(['churndep'], 1)\n",
    "Y = data['churndep']\n",
    "\n",
    "# try maximum depths from 1 to 20\n",
    "complexity_values = range(1,20)\n",
    "\n",
    "# empty list to hold my accuracies\n",
    "accuracies = []\n",
    "\n",
    "# go through each complexity_value one at a time.\n",
    "for complexity_value in complexity_values:\n",
    "    # This will create an empty tree with a specific maximum depth (complexity)\n",
    "    tree = DecisionTreeClassifier(criterion=\"entropy\", max_depth=complexity_value)\n",
    "    \n",
    "    # This will get us 10-fold cross validation accuracy with our tree and our data\n",
    "    # We can do this in one line!\n",
    "    cross_fold_accuracies = cross_val_score(tree, X, Y, scoring=\"accuracy\", cv=10)\n",
    "    \n",
    "    # Average accuracy\n",
    "    average_cross_fold_accuracy = np.mean(cross_fold_accuracies)\n",
    "    \n",
    "    # Append this accuracy to a list of accuracies\n",
    "    accuracies.append(average_cross_fold_accuracy)\n",
    "\n",
    "# We want to plot our results\n",
    "plt.plot(complexity_values, accuracies)\n",
    "plt.xlabel(\"Maximum Depth\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this analysis, what value would you set the decision tree max depth parameter to for new predictions and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answere here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Change the code above to perform a similar analysis with other measures of complexity: max leaf nodes, max features, etc. Select 3 parameters and produce 3 charts showing the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put your answer here. (step 1 - copy the code from the previous question)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7. Implement 3 algorithms and optimise parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. This question requires you to implement additional machine learning methods and improve the performance of the decision tree we analysed in the last question. Look at nearest neighbour, logistic regression (see https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), and support vector machine (see https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html).\n",
    "\n",
    "Produce graphs showing the accuracy performance for different regularisation strengths - C parameter- in logistic regression and SVM. For nearest neighbours examine the value of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here. Grading based on correct implementation and approach as well as improving the previous result. \n",
    "# you need to use cross validation and accuracy to measure the performance (as in the previous example) and try to change \n",
    "#different appropriate parameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
